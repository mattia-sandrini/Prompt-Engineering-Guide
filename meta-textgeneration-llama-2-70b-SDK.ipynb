{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af3794b",
   "metadata": {},
   "source": [
    "# Run Llama 2 Models in SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b1b24",
   "metadata": {},
   "source": [
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy a JumpStart model for Text Generation using the Llama 2 pretrained model.\n",
    "\n",
    "To perform inference on these models, you need to pass custom_attributes='accept_eula=true' as part of header. This means you have read and accept the end-user-license-agreement (EULA) of the model. EULA can be found in model card description or from https://ai.meta.com/resources/models-and-libraries/llama-downloads/. By default, this notebook sets custom_attributes='accept_eula=false', so all inference requests will fail until you explicitly change this custom attribute.\n",
    "\n",
    "Note: Custom_attributes used to pass EULA are key/value pairs. The key and value are separated by '=' and pairs are separated by ';'. If the user passes the same key more than once, the last value is kept and passed to the script handler (i.e., in this case, used for conditional logic). For example, if 'accept_eula=false; accept_eula=true' is passed to the server, then 'accept_eula=true' is kept and passed to the script handler.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c646f2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b55e677-3429-4668-b100-bd63d2a4c401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "awscli 1.27.153 requires PyYAML<5.5,>=3.10, but you have pyyaml 6.0.1 which is incompatible.\n",
      "docker-compose 1.29.2 requires PyYAML<6,>=3.10, but you have pyyaml 6.0.1 which is incompatible.\n",
      "jupyterlab 3.2.1 requires jupyter-server~=1.4, but you have jupyter-server 2.6.0 which is incompatible.\n",
      "jupyterlab 3.2.1 requires nbclassic~=0.2, but you have nbclassic 1.0.0 which is incompatible.\n",
      "jupyterlab-server 2.8.2 requires jupyter-server~=1.4, but you have jupyter-server 2.6.0 which is incompatible.\n",
      "spyder 5.1.5 requires pylint<2.10.0,>=2.5.0, but you have pylint 3.0.0a6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d458cf0-02e2-4066-927b-25fa5ef2a07e",
   "metadata": {},
   "source": [
    "***\n",
    "You can continue with the default model or choose a different model: this notebook will run with the following model IDs :\n",
    "- `meta-textgeneration-llama-2-7b`\n",
    "- `meta-textgeneration-llama-2-13b`\n",
    "- `meta-textgeneration-llama-2-70b`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b18f24",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-70b\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ff3a2f",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e52afae-868d-4736-881f-7180f393003a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id=model_id)\n",
    "predictor = model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7207e-01ba-4ac2-b4a9-c8f6f0e1c498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "***\n",
    "### Supported Parameters\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "***\n",
    "### Notes\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ca1f1dbe-e8e5-4320-96ba-8724e564269b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_response(payload, response):\n",
    "    print(payload[\"inputs\"])\n",
    "    print(f\"> {response[0]['generation']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1bf6de",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cbde5e7-1068-41f9-999a-70ef04e1cbbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I believe the meaning of life is\n",
      "> to be happy. Happiness is a choice and it comes from within. It is not dependent on others, or things. Happiness is not something that happens to you, it is something you create.\n",
      "I believe that everyone has the right to be happy, and that everyone deserves to be happy.\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 6.48 ms, sys: 0 ns, total: 6.48 ms\n",
      "Wall time: 6.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"I believe the meaning of life is\",\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3be025",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda81ccf-0188-4117-8355-801ef98aaa48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simply put, the theory of relativity states that \n",
      "> 1) the laws of physics are the same in all inertial reference frames and 2) the speed of light in a vacuum is the same for all observers, regardless of the motion of the light source.\n",
      "The theory of relativity has two parts: special relativity and general relativity.\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 3.16 ms, sys: 2.98 ms, total: 6.14 ms\n",
      "Wall time: 5.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"Simply put, the theory of relativity states that \",\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efcad4a",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de6e8250-88c8-4b1c-a70b-ae5a4976e6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A brief message congratulating the team on the launch:\n",
      "\n",
      "Hi everyone,\n",
      "\n",
      "I just \n",
      "> wanted to take a moment to congratulate everyone on the launch of\n",
      "the new website.  This is a big step forward for the company, and it\n",
      "couldn't have happened without everyone's hard work and dedication.\n",
      "\n",
      "I'm proud to be a part of such a tal\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 5.12 ms, sys: 158 µs, total: 5.28 ms\n",
      "Wall time: 5.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"\"\"A brief message congratulating the team on the launch:\n",
    "\n",
    "Hi everyone,\n",
    "\n",
    "I just \"\"\",\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9777f",
   "metadata": {},
   "source": [
    "### Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "906abf62-eee1-4bd5-9c3d-314f06a15c77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate English to French:\n",
      "sea otter => loutre de mer\n",
      "peppermint => menthe poivrée\n",
      "plush girafe => girafe peluche\n",
      "cheese =>\n",
      "> fromage\n",
      "turtle => tortue\n",
      "frog => grenouille\n",
      "parrot => perroquet\n",
      "lion => lion\n",
      "cat => chat\n",
      "elephant => éléphant\n",
      "gorilla => gorille\n",
      "dog => chien\n",
      "kangaroo => kangour\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 4.66 ms, sys: 1.67 ms, total: 6.33 ms\n",
      "Wall time: 5.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": \"\"\"Translate English to French:\n",
    "sea otter => loutre de mer\n",
    "peppermint => menthe poivrée\n",
    "plush girafe => girafe peluche\n",
    "cheese =>\"\"\",\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6086134-8af5-4164-8be6-1e7b339f6f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9f61289-1722-4ac1-b8c0-7c932b7ff025",
   "metadata": {},
   "source": [
    "# Prompt Engineering Guide\n",
    "\n",
    "https://www.promptingguide.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52755776-0b41-495e-b8f0-51c06183edc8",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39d7376e-f3ec-4d57-89af-0fa26bb0d76a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the text into \"neutral\", \"negative\" or \"positive\". \n",
      "Text: I think the vacation is okay.\n",
      "Sentiment: \n",
      "> 0.5\n",
      "\n",
      "Text: I think the vacation is great.\n",
      "Sentiment: 0.8\n",
      "\n",
      "Text: I think the vacation is terrible.\n",
      "Sentiment: 0.2\n",
      "\n",
      "Text: I think the vacation is awful.\n",
      "Sentiment: 0\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 5.39 ms, sys: 134 µs, total: 5.53 ms\n",
      "Wall time: 5.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"Classify the text into \"neutral\", \"negative\" or \"positive\". \n",
    "Text: I think the vacation is okay.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef5a5e-6680-40ce-bc31-4acd9602481d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d76ca19-112a-42f5-9b9c-6a6dbfebe319",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e58f22d3-6955-4522-a0c5-214514add113",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\n",
      "the word whatpu is:\n",
      "We were traveling in Africa and we saw these very cute whatpus.\n",
      "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\n",
      "the word farduddle is: \n",
      "> \n",
      "The children were farduddling to try to get the balloon to pop.\n",
      "To \"sneel\" means to walk backwards. An example of a sentence that uses the word sneel is:\n",
      "We were sneeling to get out of the room.\n",
      "To \"chalk\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 18.3 ms, sys: 467 µs, total: 18.8 ms\n",
      "Wall time: 5.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"A \"whatpu\" is a small, furry animal native to Tanzania. An example of a sentence that uses\n",
    "the word whatpu is:\n",
    "We were traveling in Africa and we saw these very cute whatpus.\n",
    "To do a \"farduddle\" means to jump up and down really fast. An example of a sentence that uses\n",
    "the word farduddle is: \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5dd52d9c-ae1a-4c3b-ae99-23ee00a45c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is awesome! // Negative\n",
      "This is bad! // Positive\n",
      "Wow that movie was rad! // Positive\n",
      "What a horrible show! // \n",
      "> ­Negative\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"This is awesome! // Negative\n",
    "This is bad! // Positive\n",
    "Wow that movie was rad! // Positive\n",
    "What a horrible show! // \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 4, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3ade919-6d92-4e8e-8645-faf832f80e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive This is awesome! \n",
      "This is bad! Negative\n",
      "Wow that movie was rad!\n",
      "Positive\n",
      "What a horrible show! -- \n",
      "> \n",
      "Negative\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Positive This is awesome! \n",
    "This is bad! Negative\n",
    "Wow that movie was rad!\n",
    "Positive\n",
    "What a horrible show! -- \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 4, \"top_p\": 0.9, \"temperature\": 0.1, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6868cbb-de38-47ef-90fe-5ddf5b8891bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Limitations of Few-shot Prompting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "59861b60-3fc9-4e86-83a2-9dd7dafe8a63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([n for n in [15, 32, 5, 13, 82, 7, 1] if n % 2 != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48758e38-6965-486c-8103-cd28876a7f18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does the odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1 ?\n",
      "Answer: \n",
      "> 15 + 32 + 5 + 13 + 82 + 7 + 1 = 155 = 156 - 1 = 155 = 5 * 31 = even number.\n",
      "Does the odd numbers in this group add up to an\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 5.83 ms, sys: 0 ns, total: 5.83 ms\n",
      "Wall time: 5.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"Does the odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1 ?\n",
    "Answer: \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.2, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9725ed-5552-4cbc-b9ef-fd9127da5626",
   "metadata": {},
   "source": [
    "Let's try to add some examples to see if few-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed66e46e-921e-4d6f-95ef-ba72a4e1b3c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
      "A: The answer is False.\n",
      "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
      "A: The answer is True.\n",
      "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
      "A: The answer is True.\n",
      "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
      "A: The answer is False.\n",
      "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
      "A: \n",
      "> The answer is False.\n",
      "The odd\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 5.52 ms, sys: 587 µs, total: 6.1 ms\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: The answer is False.\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: The answer is True.\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: The answer is True.\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: The answer is False.\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 8, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e90eab-8e22-4e37-bac0-bfd96ec43dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df9bbca5-a9e8-4c53-9b25-636478684af3",
   "metadata": {},
   "source": [
    "## Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "061f4502-a8e2-4aab-8650-ca4df3ad7d81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
      "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
      "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
      "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
      "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
      "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
      "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
      "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
      "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
      "A: \n",
      "> Adding all the odd numbers (15, 13, 7) gives 35. The answer is False.\n",
      "The odd numbers in this\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 4.99 ms, sys: 263 µs, total: 5.25 ms\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A: \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 32, \"top_p\": 0.9, \"temperature\": 0.3, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "822046f7-fa7f-4b0a-b395-4e662e4956cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
      "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
      "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
      "A:\n",
      "> Adding all the odd numbers (15, 13, 7, 1) gives 36. The answer is True.\n",
      "The odd numbers in this group add up to an even number: 12, 11, 5, 16, 2, 1.\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 15.7 ms, sys: 214 µs, total: 15.9 ms\n",
      "Wall time: 5.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfce9b-56a0-45fa-8854-0c3696f56a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e6e49dd3-2d19-4558-a975-135ebd4d9692",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with? \n",
      "> 10 apples\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 5.97 ms, sys: 0 ns, total: 5.97 ms\n",
      "Wall time: 530 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with? \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 4, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eb46d754-8126-41bf-9e21-bba54b6190de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
      "Let's think step by step.\n",
      "> \n",
      "So, 10-2-2+5-1=10\n",
      "Therefore, I remained with 10 apples.\n",
      "Following the order of the operations, we know that the mathematical operation to be performed first is the subtraction.\n",
      "So, we are left with:\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 4.9 ms, sys: 259 µs, total: 5.16 ms\n",
      "Wall time: 5.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
    "Let's think step by step.\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.8, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65801b6-984c-4c76-8697-7cb556cff9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fd9df91-f888-4931-8dcd-dd5cdb2413c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Self-Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e9e8f35e-808b-42d0-b094-f03362e39bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
      "Answer: \n",
      "> 35. When you were 6 your sister was 3. Now you’re 70, she’s 35.\n",
      "Question: I am a five letter word. I am often used to describe something negative. I become longer when you remove two of my letters. What word am I?\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 15.4 ms, sys: 218 µs, total: 15.6 ms\n",
      "Wall time: 5.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"Question: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "Answer: \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d334a4-ce6f-4304-bab6-c76b41f7dba2",
   "metadata": {},
   "source": [
    "The output is wrong! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e10b238-1855-4391-bed9-8f6821cb3596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "de2cf5bd-0ae0-4e56-b32e-a55e9e9eadd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
      "there will be 21 trees. How many trees did the grove workers plant today?\n",
      "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
      "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
      "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
      "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
      "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
      "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
      "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
      "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
      "did Jason give to Denny?\n",
      "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
      "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
      "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
      "he have now?\n",
      "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
      "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
      "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
      "monday to thursday. How many computers are now in the server room?\n",
      "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
      "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
      "The answer is 29.\n",
      "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
      "golf balls did he have at the end of wednesday?\n",
      "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
      "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
      "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
      "A: She bought 5 bagels for $3 each. This means she spent 5\n",
      "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
      "A: \n",
      "> 35 years old.\n",
      "Q: I’m 23 and my friend is 24. How many years ago was my friend twice as old as me?\n",
      "A: 15 years ago.\n",
      "Q: I was born in the 80’s and my friend was born in the 60’s. If in 10 years time I will be 1/3rd of her age, how old will she be?\n",
      "A: 90 years old. (Answer can be different for some values)\n",
      "Q: A bus travels 150 miles in 4\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 8.34 ms, sys: 0 ns, total: 8.34 ms\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent 5\n",
    "Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?\n",
    "A: \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 128, \"top_p\": 0.9, \"temperature\": 0.9, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e617fa8-7db1-40e8-b5ec-55bec0d96bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a742cca9-4e83-4ae0-84f4-54f559b672d0",
   "metadata": {},
   "source": [
    "## Generated Knowledge Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1bab3cc3-2ff6-4f98-9791-80387802f37b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of golf is trying to get a higher point total than others. Yes or No?\n",
      "\n",
      "> It's not\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 16.2 ms, sys: 474 µs, total: 16.7 ms\n",
      "Wall time: 447 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 4, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc3163-fa58-472f-bb17-03db06a0cf53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d535d4b3-a674-45a6-9ea2-c455d3429aab",
   "metadata": {},
   "source": [
    "First, we generate a few \"knowledges\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f1321376-e68a-4673-8e30-945397e16473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Greece is larger than mexico.\n",
      "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
      "Input: Glasses always fog up.\n",
      "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
      "Input: A fish is capable of thinking.\n",
      "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\n",
      "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
      "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
      "Input: A rock is the same size as a pebble.\n",
      "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
      "Input: Part of golf is trying to get a higher point total than others.\n",
      "Knowledge: \n",
      "> 18-hole stroke play tournaments are scored so the player with the lowest score wins.\n",
      "Input: A person can think of something that doesn’t exist.\n",
      "Knowledge: It is impossible for the human mind to think of something that doesn’t exist.\n",
      "Input: A person can think of\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 17.5 ms, sys: 747 µs, total: 18.2 ms\n",
      "Wall time: 6.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"Input: Greece is larger than mexico.\n",
    "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
    "Input: Glasses always fog up.\n",
    "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
    "Input: A fish is capable of thinking.\n",
    "Knowledge: Fish are more intelligent than they appear. In many areas, such as memory, their cognitive powers match or exceed those of ’higher’ vertebrates including non-human primates. Fish’s long-term memories help them keep track of complex social relationships.\n",
    "Input: A common effect of smoking lots of cigarettes in one’s lifetime is a higher than normal chance of getting lung cancer.\n",
    "Knowledge: Those who consistently averaged less than one cigarette per day over their lifetime had nine times the risk of dying from lung cancer than never smokers. Among people who smoked between one and 10 cigarettes per day, the risk of dying from lung cancer was nearly 12 times higher than that of never smokers.\n",
    "Input: A rock is the same size as a pebble.\n",
    "Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).\n",
    "Input: Part of golf is trying to get a higher point total than others.\n",
    "Knowledge: \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e3f89-6997-4d50-b08c-fc5c176ad70f",
   "metadata": {},
   "source": [
    "The next step is to integrate the knowledge and get a prediction. I reformatted the question into QA format to guide the answer format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "966fac2e-e623-4b99-b792-aee82e583a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
      "Knowledge: 18-hole stroke play tournaments are scored so the player with the lowest score wins.\n",
      "Explain and Answer: \n",
      "> 18-hole stroke play tournaments are scored so the player with the lowest score wins.\n",
      "Knowledge: In a 18-hole stroke\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 13.3 ms, sys: 3.97 ms, total: 17.3 ms\n",
      "Wall time: 2.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "knowledge = \"\"\" \"\"\"\n",
    "prompt = \"\"\"Question: Part of golf is trying to get a higher point total than others. Yes or No?\n",
    "Knowledge: 18-hole stroke play tournaments are scored so the player with the lowest score wins.\n",
    "Explain and Answer: \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 32, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a9cd8-665b-4d50-aaf2-80f40f9ef51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bdfc341-e41c-419b-9355-50a7dccc4125",
   "metadata": {},
   "source": [
    "## Tree of Thoughts (ToT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7176b02d-5d80-47b2-a295-4955e0cdefff",
   "metadata": {},
   "source": [
    "ToT maintains a tree of thoughts, where thoughts represent coherent language sequences that serve as intermediate steps toward solving a problem. \n",
    "This approach enables an LM to self-evaluate the progress intermediate thoughts make towards solving a problem through a deliberate reasoning process. \n",
    "The LM's ability to generate and evaluate thoughts is then combined with search algorithms (e.g., breadth-first search and depth-first search) to enable systematic exploration of thoughts with lookahead and backtracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8735d8e8-da9e-406b-a30b-f9be8436d5f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine three different experts are answering this question.\n",
      "All experts will write down 1 step of their thinking,\n",
      "then share it with the group.\n",
      "Then all experts will go on to the next step, etc.\n",
      "If any expert realises they're wrong at any point then they leave.\n",
      "The question is \"Use input numbers and basic arithmetic operations (+ - * /) to obtain 24. Each step, you are only allowed to choose two of the remaining numbers to obtain a new number. Input numbers: 4 4 6 8\".\n",
      "Answer: \n",
      "> 4*4+6=24\n",
      "Expert 1: 4*4+6=24\n",
      "Expert 2: 4*4-6=24\n",
      "Expert 3: 4*4-6-8=24\n",
      "Expert 2: 4*4-6-8=24\n",
      "Expert 1: 4*4-6-8-8=24\n",
      "Expert 2: 4*4-6-8-8-8=24\n",
      "Expert 1: 4*4-6-8-8\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 2.15 ms, sys: 3.26 ms, total: 5.41 ms\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "question = \"\"\"Use input numbers and basic arithmetic operations (+ - * /) to obtain 24. Each step, you are only allowed to choose two of the remaining numbers to obtain a new number. Input numbers: 4 4 6 8\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Imagine three different experts are answering this question.\n",
    "All experts will write down 1 step of their thinking,\n",
    "then share it with the group.\n",
    "Then all experts will go on to the next step, etc.\n",
    "If any expert realises they're wrong at any point then they leave.\n",
    "The question is \"{question}\".\n",
    "Answer: \"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 128, \"top_p\": 0.9, \"temperature\": 0.7, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b38837-60d0-4f6b-a712-488b3d08e830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9721e9a7-6085-45b6-a049-06c2accf00b7",
   "metadata": {},
   "source": [
    "## Automatic Prompt Engineer (APE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "48f73474-44b0-4bcb-b215-ba46478cb63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Here are the responses for the task assigned to the expert:\n",
    "# Demonstration start\n",
    "Input: \"prove\", Output: \"disprove\"\n",
    "Input: \"in\", Output: \"out\"\n",
    "Input: \"good\", Output: \"bad\"\n",
    "Input: \"on\", Output: \"off\"\n",
    "# Demonstration end\n",
    "\n",
    "What are instructions for this task?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a8b82bba-4688-4057-96c8-5b4bc853a920",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the responses for the task assigned to the expert:\n",
      "# Demonstration start\n",
      "Input: \"prove\", Output: \"disprove\"\n",
      "Input: \"in\", Output: \"out\"\n",
      "Input: \"good\", Output: \"bad\"\n",
      "Input: \"on\", Output: \"off\"\n",
      "# Demonstration end\n",
      "\n",
      "What are instructions for this task?\n",
      "\n",
      "> \n",
      "# Instructions start\n",
      "You are given a text string. Your task is to output the opposite of the input.\n",
      "\n",
      "For example, if the input is \"good\", the output should be \"bad\".\n",
      "\n",
      "You can assume that the input will always be a text string.\n",
      "\n",
      "# Instru\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 404 µs, sys: 5.99 ms, total: 6.39 ms\n",
      "Wall time: 5.6 s\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a2ee9d9b-e534-49bc-8db0-07ba78718780",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the responses for the task assigned to the expert:\n",
      "# Demonstration start\n",
      "Input: \"prove\", Output: \"disprove\"\n",
      "Input: \"in\", Output: \"out\"\n",
      "Input: \"good\", Output: \"bad\"\n",
      "Input: \"on\", Output: \"off\"\n",
      "# Demonstration end\n",
      "\n",
      "What are instructions for this task?\n",
      "\n",
      "> \n",
      "# Instructions start\n",
      "For each input word, return the corresponding output word.\n",
      "\n",
      "Input words will be provided as a single string.\n",
      "\n",
      "# Instructions end\n",
      "\n",
      "# Tests start\n",
      "# Test 1\n",
      "Input: \"prove\"\n",
      "Output: \"disprove\"\n",
      "\n",
      "\n",
      "==================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b54790-c3d4-4b93-aaf1-6bec63abb7ba",
   "metadata": {},
   "source": [
    "The best candidate is:\n",
    "```\n",
    "You are given a text string. Your task is to output the opposite of the input.\n",
    "\n",
    "For example, if the input is \"good\", the output should be \"bad\".\n",
    "\n",
    "You can assume that the input will always be a text string.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a68dc-f446-4a6d-afac-1c30db008f5b",
   "metadata": {},
   "source": [
    "Kerned died. Need to recreate the predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18926897-5627-43d7-912c-c37f4b85e1c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "model_id, model_version = \"meta-textgeneration-llama-2-70b\", \"*\"\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=\"meta-textgeneration-llama-2-70b-2023-07-31-15-48-33-864\", \n",
    "    serializer=sagemaker.serializers.retrieve_default(model_id=model_id, model_version=model_version), \n",
    "    deserializer=sagemaker.deserializers.retrieve_default(model_id=model_id, model_version=model_version)\n",
    ")\n",
    "predictor.content_type = sagemaker.content_types.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "predictor.accept = sagemaker.accept_types.retrieve_default(model_id=model_id, model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c90f282-cf77-41fd-adde-c6f5ce9f972f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a text string. Your task is to output the opposite of the input.\n",
      "For example, if the input is \"good\", the output should be \"bad\".\n",
      "Input: direct\n",
      "Output: \n",
      "\n",
      "> indirect\n",
      "\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 5.31 ms, sys: 0 ns, total: 5.31 ms\n",
      "Wall time: 415 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = \"\"\"You are given a text string. Your task is to output the opposite of the input.\n",
    "For example, if the input is \"good\", the output should be \"bad\".\n",
    "Input: direct\n",
    "Output: \n",
    "\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\"max_new_tokens\": 8, \"top_p\": 0.9, \"temperature\": 0.9, \"return_full_text\": False}\n",
    "}\n",
    "\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_response(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdc0c7-1d51-4c8d-9acf-57eea03dd846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b665346-5d40-4836-9ea2-42d4a433f3fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e062d29",
   "metadata": {},
   "source": [
    "## Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24cc5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9d795-96a9-4399-b070-7d74dea01f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
